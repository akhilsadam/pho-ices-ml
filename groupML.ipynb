{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "e3e15b42-766b-4240-8049-a4c01d20e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import default_rng as rg\n",
    "%matplotlib inline\n",
    "backend = plt.get_backend()\n",
    "import jpcm\n",
    "plt.switch_backend(backend)\n",
    "cs = jpcm.get('fuyu').resampled(10)\n",
    "rng = rg(12345)\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "cae80a7b-7a78-48f4-884e-19092c234772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Make a group representation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "762e7c5e-b459-4785-acba-06b877943704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(x):\n",
    "    return x #torch.floor(x) #1/(1+torch.exp(-x))\n",
    "def inv_act(x):\n",
    "    return x #torch.log(x/(1-x))\n",
    "\n",
    "\n",
    "# base FCN AE, 4 layer\n",
    "class AE(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AE,self).__init__()\n",
    "        self.activation = act##torch.nn.functional.sigmoid #functional.relu #\n",
    "        self.invactivation = inv_act#torch.nn.functional.sigmoid #functional.relu ##\n",
    "        p = 12\n",
    "        self.rep_shape = [2,2]\n",
    "        self.rep_n = self.rep_shape[0] * self.rep_shape[1]\n",
    "        self.dim = self.rep_n * 2\n",
    "        # encoder\n",
    "        self.linear1e = torch.nn.Linear(2, p)\n",
    "        self.linear2e = torch.nn.Linear(p, p)\n",
    "        self.linear3e = torch.nn.Linear(p, p)\n",
    "        self.linear4e = torch.nn.Linear(p, p)\n",
    "        self.linear5e = torch.nn.Linear(p, self.dim)\n",
    "        \n",
    "        # decoder\n",
    "        self.linear5d = torch.nn.Linear(self.dim, p)\n",
    "        self.linear4d = torch.nn.Linear(p, p)\n",
    "        self.linear3d = torch.nn.Linear(p, p)\n",
    "        self.linear2d = torch.nn.Linear(p, p)\n",
    "        self.linear1d = torch.nn.Linear(p, 2)\n",
    "    \n",
    "    def forward_e(self, x):\n",
    "        x = self.linear1e(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2e(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear3e(x)  \n",
    "        x = self.activation(x)\n",
    "        x = self.linear4e(x)  \n",
    "        x = self.activation(x)\n",
    "        x = self.linear5e(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_d(self, x):\n",
    "        x = self.linear5d(x)\n",
    "        x = self.invactivation(x)\n",
    "        x = self.linear4d(x)\n",
    "        x = self.invactivation(x)  \n",
    "        x = self.linear3d(x)\n",
    "        x = self.invactivation(x)\n",
    "        x = self.linear2d(x)\n",
    "        x = self.invactivation(x)\n",
    "        x = self.linear1d(x)        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.forward_e(x1)\n",
    "        x2 = self.forward_e(x2)\n",
    "        # complex matrix multiplication\n",
    "        r1 = x1[:self.rep_n].reshape(self.rep_shape)\n",
    "        i1 = x1[self.rep_n:].reshape(self.rep_shape)\n",
    "        r2 = x2[:self.rep_n].reshape(self.rep_shape)\n",
    "        i2 = x2[self.rep_n:].reshape(self.rep_shape)\n",
    "        #\n",
    "        r3 = r1 @ r2 - i1 @ i2\n",
    "        i3 = r1 @ i2 + i1 @ r2\n",
    "        #\n",
    "        x3 = torch.hstack([r3.flatten(),i3.flatten()])\n",
    "        return self.forward_d(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "81822a12-f47f-4fa8-86e0-f777771a0e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod AE, 4 layer\n",
    "class AE_M(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AE_M,self).__init__()\n",
    "\n",
    "        self.activation = torch.nn.functional.sigmoid #functional.relu #\n",
    "        self.invactivation = torch.nn.functional.sigmoid #functional.relu ##\n",
    "        self.acty = lambda x: x%2\n",
    "        self.actz = lambda x: x%6\n",
    "        p = 12\n",
    "        l = 6\n",
    "        \n",
    "        self.rep_shape = [2,2]\n",
    "        self.rep_n = self.rep_shape[0] * self.rep_shape[1]\n",
    "        self.dim= self.rep_n * 2\n",
    "        # encoder  \n",
    "        self.linear1e = torch.nn.Linear(2, p)\n",
    "        self.linear2e = torch.nn.Linear(p, l)\n",
    "        self.linear3e = torch.nn.Linear(p, p-l)\n",
    "        self.linear4e = torch.nn.Linear(p, p)\n",
    "        self.linear5e = torch.nn.Linear(p, self.dim)\n",
    "        \n",
    "        # decoder\n",
    "        self.linear5d = torch.nn.Linear(self.dim, p)\n",
    "        self.linear4d = torch.nn.Linear(p, l)\n",
    "        self.linear3d = torch.nn.Linear(p, p-l)\n",
    "        self.linear2d = torch.nn.Linear(p, p)\n",
    "        self.linear1d = torch.nn.Linear(p, 2)\n",
    "    \n",
    "    def forward_e(self, x):\n",
    "        x = self.linear1e(x)\n",
    "        x = self.activation(x)\n",
    "        y = self.linear2e(x)\n",
    "        y = self.acty(y)\n",
    "        z = self.linear3e(x)  \n",
    "        z = self.actz(z)\n",
    "        x = self.linear4e(torch.hstack([y,z]))  \n",
    "        x = self.activation(x)\n",
    "        x = self.linear5e(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_d(self, x):\n",
    "        x = self.linear5d(x)\n",
    "        x = self.invactivation(x)\n",
    "        y = self.linear4d(x)\n",
    "        y = self.acty(y)  \n",
    "        z = self.linear3d(x)\n",
    "        z = self.actz(z)\n",
    "        x = self.linear2d(torch.hstack([y,z]))\n",
    "        x = self.invactivation(x)\n",
    "        x = self.linear1d(x)        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.forward_e(x1)\n",
    "        x2 = self.forward_e(x2)\n",
    "        # complex matrix multiplication\n",
    "        r1 = x1[:self.rep_n].reshape(self.rep_shape)\n",
    "        i1 = x1[self.rep_n:].reshape(self.rep_shape)\n",
    "        r2 = x2[:self.rep_n].reshape(self.rep_shape)\n",
    "        i2 = x2[self.rep_n:].reshape(self.rep_shape)\n",
    "        #\n",
    "        r3 = r1 @ r2 - i1 @ i2\n",
    "        i3 = r1 @ i2 + i1 @ r2\n",
    "        #\n",
    "        x3 = torch.hstack([r3.flatten(),i3.flatten()])\n",
    "        return self.forward_d(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "dff66b48-0b61-43ed-b277-1d16efcf5f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base CNN AE, 4 layer\n",
    "class AE_C(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AE_C,self).__init__()\n",
    "\n",
    "        self.activation = torch.nn.functional.sigmoid #functional.relu #\n",
    "        self.invactivation = torch.nn.functional.sigmoid #functional.relu ##\n",
    "        p = 20\n",
    "        self.rep_shape = [2,2]\n",
    "        self.rep_n = self.rep_shape[0] * self.rep_shape[1]\n",
    "        self.dim = self.rep_n * 2\n",
    "        kernel_size = 5\n",
    "        # encoder\n",
    "        self.linear1e = torch.nn.Linear(2, p)\n",
    "        self.linear2e = torch.nn.Conv1d(1,1, kernel_size=kernel_size, padding='same', bias=True)\n",
    "        self.linear3e = torch.nn.Conv1d(1,1, kernel_size=kernel_size, padding='same', dilation=2, bias=True)\n",
    "        self.linear4e = torch.nn.Linear(p, p)\n",
    "        self.linear5e = torch.nn.Linear(p, self.dim)\n",
    "        \n",
    "        # decoder\n",
    "        self.linear5d = torch.nn.Linear(self.dim, p)\n",
    "        self.linear4d = torch.nn.Linear(p, p)\n",
    "        self.linear3d = torch.nn.Conv1d(1,1, kernel_size=kernel_size, padding='same', bias=True)\n",
    "        self.linear2d = torch.nn.Conv1d(1,1, kernel_size=kernel_size, padding='same', dilation=2, bias=True)\n",
    "        self.linear1d = torch.nn.Linear(p, 2)\n",
    "    \n",
    "    def forward_e(self, x):\n",
    "        x = self.linear1e(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2e(x.reshape(1, 1, -1))\n",
    "        x = self.activation(x)\n",
    "        x = self.linear3e(x)  \n",
    "        x = self.activation(x).reshape(-1)\n",
    "        x = self.linear4e(x)  \n",
    "        x = self.activation(x)\n",
    "        x = self.linear5e(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_d(self, x):\n",
    "        x = self.linear5d(x)\n",
    "        x = self.invactivation(x)\n",
    "        x = self.linear4d(x)\n",
    "        x = self.invactivation(x)  \n",
    "        x = self.linear3d(x.reshape(1, 1, -1))\n",
    "        x = self.invactivation(x)\n",
    "        x = self.linear2d(x)\n",
    "        x = self.invactivation(x).reshape(-1)\n",
    "        x = self.linear1d(x)        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.forward_e(x1)\n",
    "        x2 = self.forward_e(x2)\n",
    "        # complex matrix multiplication\n",
    "        r1 = x1[:self.rep_n].reshape(self.rep_shape)\n",
    "        i1 = x1[self.rep_n:].reshape(self.rep_shape)\n",
    "        r2 = x2[:self.rep_n].reshape(self.rep_shape)\n",
    "        i2 = x2[self.rep_n:].reshape(self.rep_shape)\n",
    "        #\n",
    "        r3 = r1 @ r2 - i1 @ i2\n",
    "        i3 = r1 @ i2 + i1 @ r2\n",
    "        #\n",
    "        x3 = torch.hstack([r3.flatten(),i3.flatten()])\n",
    "        return self.forward_d(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "f1d8a0e0-d741-48ac-ac6a-5f81dd316777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def primes():\n",
    "    for p in [2,3,5,7]: yield p                 # base wheel primes\n",
    "    gaps1 = [ 2,4,2,4,6,2,6,4,2,4,6,6,2,6,4,2,6,4,6,8,4,2,4,2,4,8 ]\n",
    "    gaps = gaps1 + [ 6,4,6,2,4,6,2,6,6,4,2,4,6,2,6,4,2,4,2,10,2,10 ] # wheel2357\n",
    "    def wheel_prime_pairs():\n",
    "        yield (11,0); bps = wheel_prime_pairs() # additional primes supply\n",
    "        p, pi = next(bps); q = p * p            # adv to get 11 sqr'd is 121 as next square to put\n",
    "        sieve = {}; n = 13; ni = 1              #   into sieve dict; init cndidate, wheel ndx\n",
    "        while True:\n",
    "            if n not in sieve:                  # is not a multiple of previously recorded primes\n",
    "                if n < q: yield (n, ni)         # n is prime with wheel modulo index\n",
    "                else:\n",
    "                    npi = pi + 1                # advance wheel index\n",
    "                    if npi > 47: npi = 0\n",
    "                    sieve[q + p * gaps[pi]] = (p, npi) # n == p * p: put next cull position on wheel\n",
    "                    p, pi = next(bps); q = p * p  # advance next prime and prime square to put\n",
    "            else:\n",
    "                s, si = sieve.pop(n)\n",
    "                nxt = n + s * gaps[si]          # move current cull position up the wheel\n",
    "                si = si + 1                     # advance wheel index\n",
    "                if si > 47: si = 0\n",
    "                while nxt in sieve:             # ensure each entry is unique by wheel\n",
    "                    nxt += s * gaps[si]\n",
    "                    si = si + 1                 # advance wheel index\n",
    "                    if si > 47: si = 0\n",
    "                sieve[nxt] = (s, si)            # next non-marked multiple of a prime\n",
    "            nni = ni + 1                        # advance wheel index\n",
    "            if nni > 47: nni = 0\n",
    "            n += gaps[ni]; ni = nni             # advance on the wheel\n",
    "    for p, pi in wheel_prime_pairs(): yield p   # strip out indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "7ea57bb8-10b4-4cc4-8682-6343a01e7db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RootCell(torch.nn.Module):\n",
    "    def __init__(self, n, use_primes=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        if use_primes:\n",
    "            self.p = torch.from_numpy(np.fromiter(primes(),float,count=n)).float()\n",
    "        else:\n",
    "            self.p = torch.from_numpy(np.arange(1,n+1)).float()\n",
    "\n",
    "    def forward(self, x, w):\n",
    "        # w_times_x= torch.mm(x, self.weights.t())\n",
    "        # torch.add(w_times_x, self.bias)  # w times x + b\n",
    "        roots_r = torch.cos(2*np.pi * x / self.p) * w\n",
    "        roots_i = torch.sin(2*np.pi * x / self.p) * w\n",
    "        return roots_r, roots_i      \n",
    "\n",
    "class PowerCell(torch.nn.Module):\n",
    "    def __init__(self, n, use_primes=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        if use_primes:\n",
    "            self.p = torch.from_numpy(np.fromiter(primes(),float,count=n)).float()\n",
    "        else:\n",
    "            self.p = torch.from_numpy(np.arange(1,n+1)).float()\n",
    "\n",
    "    def forward(self, x_r, x_i, w):\n",
    "        order = torch.abs(2*np.pi / torch.atan2(x_i,x_r))\n",
    "        cypow = (self.p / order) * w\n",
    "        return cypow\n",
    "        \n",
    "    \n",
    "# rt = RootCell(12)\n",
    "# rr,ri = rt(torch.Tensor([1]*12))\n",
    "# pc = PowerCell(12)\n",
    "# assert torch.mean(pc(rr,ri)) - 1.0 < 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "id": "bfbe3f32-0602-40f1-8616-e859f593ce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE_G(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AE_G,self).__init__()\n",
    "        p = 10\n",
    "        \n",
    "        cgroup = torch.ones(p)\n",
    "        self.weights = torch.nn.Parameter(cgroup)\n",
    "        \n",
    "        # base\n",
    "        self.rt1 = RootCell(p,use_primes=False)\n",
    "        self.pc1 = PowerCell(p,use_primes=False)\n",
    "        \n",
    "        # encoder\n",
    "        self.linear1a = torch.nn.Linear(2, p)\n",
    "        self.linear1b = torch.nn.Linear(2, p)\n",
    "        self.linear1e = torch.nn.Linear(2*p,p)\n",
    "        # decoder\n",
    "        self.linear1d = torch.nn.Linear(p, 2)\n",
    "        \n",
    "    def forward(self,x1, x2):\n",
    "        x1 = self.linear1a(x1)\n",
    "        x2 = self.linear1b(x2)\n",
    "        x = self.linear1e(torch.hstack([x1,x2]))\n",
    "        rr,ri = self.rt1(x,w=self.weights)\n",
    "        mod = self.pc1(rr,ri,w=self.weights)\n",
    "        out = self.linear1d(mod)\n",
    "        return out\n",
    "\n",
    "class AE_G2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AE_G2,self).__init__()\n",
    "        p = 10\n",
    "        \n",
    "        cgroup = torch.ones(p)\n",
    "        self.weights = torch.nn.Parameter(cgroup)\n",
    "        \n",
    "        # base\n",
    "        self.rt1 = RootCell(p,use_primes=False)\n",
    "        self.pc1 = PowerCell(p,use_primes=False)\n",
    "        \n",
    "        # encoder\n",
    "        self.linear1a = torch.nn.Linear(2, p)\n",
    "        self.linear1b = torch.nn.Linear(2, p) # create powers of unity\n",
    "        # decoder\n",
    "        self.linear1d = torch.nn.Linear(p, 2)\n",
    "        \n",
    "    def forward(self,x1, x2):\n",
    "        \n",
    "        x1 = self.linear1a(x1) # encode\n",
    "        rr1,ri1 = self.rt1(x1,w=self.weights)\n",
    "        \n",
    "        x2 = self.linear1b(x2) # encode\n",
    "        rr2,ri2 = self.rt1(x2,w=self.weights)\n",
    "        \n",
    "        r3 = rr1 @ rr2 - ri1 @ ri2\n",
    "        i3 = rr1 @ ri2 + ri1 @ rr2\n",
    "                \n",
    "        mod = self.pc1(r3,i3,w=self.weights)\n",
    "        out = self.linear1d(mod)\n",
    "        return out  \n",
    "    \n",
    "class AE_G3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AE_G3,self).__init__()\n",
    "        p = 10\n",
    "        rep = 2\n",
    "        self.rep_shape=[rep,rep]\n",
    "        cgroup = torch.ones(p)\n",
    "        self.weights = torch.nn.Parameter(cgroup)\n",
    "        \n",
    "        # base\n",
    "        self.rt1 = RootCell(p,use_primes=False)\n",
    "        self.pc1 = PowerCell(p,use_primes=False)\n",
    "        \n",
    "        # encoder\n",
    "        self.linear1 = torch.nn.Linear(2, p)\n",
    "        self.linear2 = torch.nn.Linear(p,rep**2) # map to matrix rep\n",
    "        \n",
    "        # decoder\n",
    "        self.linear2inv = torch.nn.Linear(rep**2,p)\n",
    "        self.linear1d = torch.nn.Linear(p, 2)\n",
    "        \n",
    "    def forward(self,x1, x2):\n",
    "        \n",
    "        x1 = self.linear1(x1) # encode\n",
    "        rr1,ri1 = self.rt1(x1,w=self.weights)\n",
    "        rr1 = self.linear2(rr1)\n",
    "        ri1 = self.linear2(ri1)\n",
    "        r1 = rr1.reshape(self.rep_shape)\n",
    "        i1 = ri1.reshape(self.rep_shape)\n",
    "        \n",
    "        x2 = self.linear1(x2) # encode\n",
    "        rr2,ri2 = self.rt1(x2,w=self.weights)\n",
    "        rr2 = self.linear2(rr2)\n",
    "        ri2 = self.linear2(ri2)\n",
    "        r2 = rr2.reshape(self.rep_shape)\n",
    "        i2 = ri2.reshape(self.rep_shape)\n",
    "        \n",
    "        r3 = r1 @ r2 - i1 @ i2 # mult\n",
    "        i3 = r1 @ i2 + i1 @ r2\n",
    "                \n",
    "        r4 = self.linear2inv(r3.flatten()) # decode\n",
    "        i4 = self.linear2inv(i3.flatten())\n",
    "        mod = self.pc1(r4,i4,w=self.weights)\n",
    "        out = self.linear1d(mod)\n",
    "        return out    \n",
    "    \n",
    "class AE_G4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AE_G4,self).__init__()\n",
    "        \n",
    "        p = 10            # possible orders (2-p)\n",
    "        self.rep = 2           # dimension of C matrix rep\n",
    "        internal_mult = 2 # number of generators (cyclic)\n",
    "        \n",
    "        self.internal_mult = 2 \n",
    "        \n",
    "        \n",
    "        self.rep2 = self.rep**2\n",
    "        self.rep_shape=[self.rep,self.rep]\n",
    "        \n",
    "        cgroup = torch.ones(p)\n",
    "        ogroup = torch.ones(p)\n",
    "        self.weights = torch.nn.Parameter(cgroup) # a multiplicative selection of generators\n",
    "        self.orders = torch.nn.Parameter(ogroup) # the powers of each generator (which would be one unless otherwise necessary for the complex rep)\n",
    "        \n",
    "        # base\n",
    "        self.rt = RootCell(p,use_primes=False)\n",
    "        self.pc = PowerCell(p,use_primes=False)\n",
    "        \n",
    "        # encoder\n",
    "        self.linear1 = torch.nn.Linear(2, self.internal_mult)\n",
    "        self.maps = [torch.nn.Linear(p,self.rep2) for _ in range(self.internal_mult)] # map to matrix rep\n",
    "        \n",
    "        # decoder\n",
    "        self.linear2inv = torch.nn.Linear(self.rep2,p)\n",
    "        self.linear1d = torch.nn.Linear(p, 2)\n",
    "        \n",
    "    def generate(self):\n",
    "        rp = []\n",
    "        ip = []\n",
    "        rr, ri = self.rt(self.orders,w=self.weights)\n",
    "        for i in range(self.internal_mult):\n",
    "            rr1 = self.maps[i](rr)\n",
    "            ri1 = self.maps[i](ri)\n",
    "            r = rr1.reshape(self.rep_shape)\n",
    "            i = ri1.reshape(self.rep_shape)\n",
    "            rp.append(r)\n",
    "            ip.append(i)\n",
    "        return rp, ip\n",
    "        \n",
    "\n",
    "    def encode(self,x,rp,ip):\n",
    "        x = self.linear1(x)\n",
    "        ra = torch.eye(self.rep)\n",
    "        ia = torch.zeros(self.rep_shape)\n",
    "        for j in range(self.internal_mult):\n",
    "            for i in range(int(x[j])):\n",
    "                ra, ia = self.mul(ra,ia,rp[j],ip[j])\n",
    "        return ra, ia\n",
    "    \n",
    "    def decode(self,r,i):\n",
    "        r4 = self.linear2inv(r.flatten()) # decode\n",
    "        i4 = self.linear2inv(i.flatten())\n",
    "        \n",
    "        mod = self.pc(r4,i4,w=self.weights)\n",
    "        out = self.linear1d(mod)\n",
    "        return out\n",
    "    \n",
    "    def mul(self,r1,i1,r2,i2):\n",
    "        r3 = r1 @ r2 - i1 @ i2\n",
    "        i3 = r1 @ i2 + i1 @ r2\n",
    "        return r3,i3\n",
    "        \n",
    "    def forward(self,x1, x2):\n",
    "        \n",
    "        # make generators\n",
    "        rp,ip = self.generate()\n",
    "        \n",
    "        # encode, decode\n",
    "        \n",
    "        r1, i1 = self.encode(x1,rp,ip)\n",
    "        r2, i2 = self.encode(x2,rp,ip)\n",
    "        r3, i3, = self.mul(r1,i1,r2,i2)\n",
    "        out = self.decode(r3,i3)               \n",
    "\n",
    "        return out    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3ab9cb-f761-4811-9110-33720493decc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "id": "f36d412b-85bc-45cc-8dfc-0a974366aafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example : dihedral, D6\n",
    "def sample(n):\n",
    "    x = rng.integers(low=0,high=6,size=n)\n",
    "    y = rng.integers(low=0,high=2,size=n)\n",
    "    return np.vstack([x,y]).T\n",
    "def mult(e1,e2):\n",
    "    x1 = e1[:,0]\n",
    "    y1 = e1[:,1]\n",
    "    x2 = e2[:,0]\n",
    "    y2 = e2[:,1]\n",
    "    x = (x1 - x2) % 6\n",
    "    y = (y1 + y2) % 2\n",
    "    return np.vstack([x,y]).T\n",
    "# example2 : c6 x 1\n",
    "def sample2(n):\n",
    "    x = rng.integers(low=0,high=6,size=n)\n",
    "    y = np.ones(n)\n",
    "    return np.vstack([x,y]).T\n",
    "def mult2(e1,e2):\n",
    "    x1 = e1[:,0]\n",
    "    y1 = e1[:,1]\n",
    "    x2 = e2[:,0]\n",
    "    y2 = e2[:,1]\n",
    "    x = (x1 + x2) % 6\n",
    "    y = np.ones(y1.shape)\n",
    "    return np.vstack([x,y]).T\n",
    "# def any_sample(n):\n",
    "#     x = rng.integers(low=0,high=10,size=n)\n",
    "#     y = rng.integers(low=0,high=10,size=n)\n",
    "#     return np.vstack([x,y]).T\n",
    "# def valid(data):\n",
    "#     x = data[:,0]\n",
    "#     y = data[:,1]\n",
    "#     return np.where(x<6,True,False)*np.where(y<2,True,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "id": "7fd5fcd6-b2a0-4b16-841b-c0ef4992e88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\" # torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # as other apps are currently using gpu\n",
    "\n",
    "n = 4000\n",
    "tn = 5\n",
    "\n",
    "x1 = torch.Tensor(sample(n+tn)).to(device)\n",
    "x2 = torch.Tensor(sample(n+tn)).to(device)\n",
    "y  = torch.Tensor(mult(x1,x2)).to(device)\n",
    "\n",
    "\n",
    "# x1 = torch.Tensor(sample2(n+tn)).to(device)\n",
    "# x2 = torch.Tensor(sample2(n+tn)).to(device)\n",
    "# y  = torch.Tensor(mult2(x1,x2)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "id": "22b3fbff-81e0-4359-8c1e-33fe9446a96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 24/24 [01:18<00:00,  3.26s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size=30\n",
    "\n",
    "# AE_G() is modular arithmetic, known to work\n",
    "# AE_G2() is an actual rep, but without matrices...\n",
    "# AE_G3() is a proper C 2x2 matrix rep, but can only represent cyclic groups...\n",
    "# AE_G4() is a proper C 2x2 matrix rep, can represent cyclic and dihedral\n",
    "\n",
    "nets = [AE_G4()]  # AE(), AE_M(), AE_C(), do not work\n",
    "err = []\n",
    "for net in nets:\n",
    "    net.to(device)\n",
    "    criterion = torch.nn.MSELoss() #CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.002, momentum=0.9)\n",
    "    # optimizer = optim.ASGD(net.parameters(), lr=0.002)\n",
    "    for epoch in tqdm(range(24)): \n",
    "        running_loss = 0.0\n",
    "        for i in range(n//batch_size):\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "\n",
    "            outputs = torch.vstack([net(x1[j],x2[j]) for j in range(i,i+batch_size)])\n",
    "            loss = criterion(outputs, y[i:i+batch_size])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        # print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
    "        # running_loss = 0.0\n",
    "    err.append(criterion(torch.vstack([net(x1i,x2i) for x1i,x2i in zip(x1[n:],x2[n:])]),y[n:]).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "id": "bbb81bd2-a057-4665-b8d9-b3334db2b062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 892,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(f'Net: {type(n).__name__} has test MSE error e={e}') for n,e in zip(nets,err)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "id": "7327e11e-9a22-4e11-b9a6-fb2fa8a930c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([2.0856, 0.5882], grad_fn=<AddBackward0>),\n",
       " tensor([2.0856, 0.5882], grad_fn=<AddBackward0>),\n",
       " tensor([2.0856, 0.5882], grad_fn=<AddBackward0>),\n",
       " tensor([2.0856, 0.5882], grad_fn=<AddBackward0>),\n",
       " tensor([2.0856, 0.5882], grad_fn=<AddBackward0>)]"
      ]
     },
     "execution_count": 885,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[nets[0](x1i,x2i) for x1i,x2i in zip(x1[n:],x2[n:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "id": "8bf53ad8-39d6-44e9-bff1-abc3823956e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 0.],\n",
       "        [5., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [4., 1.]])"
      ]
     },
     "execution_count": 886,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "id": "f9e58ae3-0eb4-4e5c-9737-be362051561f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net: AE_G4 has weights=Parameter containing:\n",
      "tensor([0.9805, 0.9885, 0.9344, 0.8784, 0.7432, 0.6506, 0.8213, 0.5637, 0.5261,\n",
      "        0.8257], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 887,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(f'Net: {type(n).__name__} has weights={n.weights}') for n in nets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f160d46a-9c38-49df-bd2e-ebf308052888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
